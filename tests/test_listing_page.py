#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ª–æ—Ç–∞.
"""
import asyncio
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent))

from parsers.item_page_parser import ItemPageParser
from core.steam_parser import SteamMarketParser
from core.config import Config
from services.redis_service import RedisService
from loguru import logger

async def test_listing_page():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º listing_id."""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger.remove()
    logger.add(sys.stderr, level="DEBUG", format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
    # RedisService —Å–æ–∑–¥–∞–µ—Ç—Å—è –∏–∑ REDIS_URL
    redis_service = None
    if Config.REDIS_ENABLED:
        redis_service = RedisService(Config.REDIS_URL)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä (–±–µ–∑ proxy_manager –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è)
    parser = SteamMarketParser(
        redis_service=redis_service
    )
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∞
    appid = 730
    hash_name = "AK-47 | Redline (Battle-Scarred)"
    target_listing_id = "733651971153157038"
    
    logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è: {hash_name}")
    logger.info(f"üéØ –ò—â–µ–º listing_id: {target_listing_id}")
    
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    logger.info("üìÑ –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏...")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
    html = await parser._fetch_item_page(appid, hash_name, page=1)
    if not html:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
        return
    
    page_parser = ItemPageParser(html)
    all_listings = page_parser.get_all_listings()
    logger.info(f"üìã –°—Ç—Ä–∞–Ω–∏—Ü–∞ 1: –ù–∞–π–¥–µ–Ω–æ {len(all_listings)} –ª–æ—Ç–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–∞—à listing_id –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    found_on_page = None
    for listing in all_listings:
        if listing.get('listing_id') == target_listing_id:
            found_on_page = 1
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 1!")
            logger.info(f"   –¶–µ–Ω–∞: ${listing.get('price', 'N/A'):.2f}")
            logger.info(f"   Inspect —Å—Å—ã–ª–∫–∞: {listing.get('inspect_link', 'N/A')}")
            logger.info(f"   Listing ID: {listing.get('listing_id', 'N/A')}")
            break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ç–æ—Ä—É—é
    if not found_on_page:
        logger.info("üìÑ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É 2...")
        await asyncio.sleep(1)
        html_page2 = await parser._fetch_item_page(appid, hash_name, page=2)
        if html_page2:
            page_parser2 = ItemPageParser(html_page2)
            all_listings_page2 = page_parser2.get_all_listings()
            logger.info(f"üìã –°—Ç—Ä–∞–Ω–∏—Ü–∞ 2: –ù–∞–π–¥–µ–Ω–æ {len(all_listings_page2)} –ª–æ—Ç–æ–≤")
            
            for listing in all_listings_page2:
                if listing.get('listing_id') == target_listing_id:
                    found_on_page = 2
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 2!")
                    logger.info(f"   –¶–µ–Ω–∞: ${listing.get('price', 'N/A'):.2f}")
                    logger.info(f"   Inspect —Å—Å—ã–ª–∫–∞: {listing.get('inspect_link', 'N/A')}")
                    logger.info(f"   Listing ID: {listing.get('listing_id', 'N/A')}")
                    break
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    if not found_on_page:
        logger.info("üìÑ –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã...")
        total_count = page_parser.get_total_listings_count()
        logger.info(f"üìä –í—Å–µ–≥–æ –ª–æ—Ç–æ–≤: {total_count}")
        
        listings_per_page = 10
        if total_count:
            total_pages = (total_count + listings_per_page - 1) // listings_per_page
            logger.info(f"üìä –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
            max_pages = min(total_pages + 1, 20)
        else:
            # –ï—Å–ª–∏ total_count –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º –º–µ–Ω—å—à–µ 10 –ª–æ—Ç–æ–≤
            logger.info("üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ —Ç–µ—Ö –ø–æ—Ä, –ø–æ–∫–∞ –Ω–µ –ø–æ–ª—É—á–∏–º –º–µ–Ω—å—à–µ 10 –ª–æ—Ç–æ–≤")
            max_pages = 20
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü, —Ç–∞–∫ –∫–∞–∫ –ª–æ—Ç —Å —Ü–µ–Ω–æ–π $38.92 –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–∞–ª—å—à–µ
        for page in range(2, max_pages):
            await asyncio.sleep(1)
            html_page = await parser._fetch_item_page(appid, hash_name, page=page)
            if html_page:
                page_parser_page = ItemPageParser(html_page)
                page_listings = page_parser_page.get_all_listings()
                logger.info(f"üìã –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ù–∞–π–¥–µ–Ω–æ {len(page_listings)} –ª–æ—Ç–æ–≤")
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ 10 –ª–æ—Ç–æ–≤ –∏ total_count –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω - —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                if not total_count and len(page_listings) < 10:
                    logger.info(f"üìã –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ü–æ–ª—É—á–µ–Ω–æ –º–µ–Ω—å—à–µ 10 –ª–æ—Ç–æ–≤, —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                
                for listing in page_listings:
                    if listing.get('listing_id') == target_listing_id:
                        found_on_page = page
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}!")
                        logger.info(f"   –¶–µ–Ω–∞: ${listing.get('price', 'N/A'):.2f}")
                        logger.info(f"   Inspect —Å—Å—ã–ª–∫–∞: {listing.get('inspect_link', 'N/A')}")
                        logger.info(f"   Listing ID: {listing.get('listing_id', 'N/A')}")
                        break
                
                if found_on_page:
                    break
                
                # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ –º–µ–Ω—å—à–µ 10 –ª–æ—Ç–æ–≤ –∏ total_count –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω - –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
                if not total_count and len(page_listings) < 10:
                    break
    
    if not found_on_page:
        logger.error(f"‚ùå Listing ID {target_listing_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö")
        logger.info("üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º HTML –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —ç—Ç–æ–≥–æ listing_id...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Ç–æ—Ä—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º HTML
        html_page2 = await parser._fetch_item_page(appid, hash_name, page=2)
        if html_page2:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_page2, 'html.parser')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ listing_id –≤ HTML –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
            if target_listing_id in html_page2:
                logger.info(f"‚úÖ Listing ID {target_listing_id} –Ω–∞–π–¥–µ–Ω –≤ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2 –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞!")
            else:
                logger.warning(f"‚ö†Ô∏è Listing ID {target_listing_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã 2 –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞")
            
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å —ç—Ç–∏–º listing_id –≤ –∞—Ç—Ä–∏–±—É—Ç–µ id
            listing_row = soup.find('div', id=f'listing_{target_listing_id}')
            if listing_row:
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç div#listing_{target_listing_id}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ü–µ–Ω–∞
                price_elem = listing_row.select_one('.market_listing_price_with_fee')
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    logger.info(f"   –¶–µ–Ω–∞ –Ω–∞–π–¥–µ–Ω–∞: {price_text}")
                else:
                    logger.warning("   ‚ö†Ô∏è –¶–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ")
                    # –ü—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
                    price_elems = listing_row.select('.market_listing_price, .normal_price')
                    for price_elem in price_elems:
                        price_text = price_elem.get_text(strip=True)
                        logger.info(f"   –¶–µ–Ω–∞ (fallback): {price_text}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ inspect —Å—Å—ã–ª–∫–∞
                inspect_elem = listing_row.find('a', href=lambda x: x and 'csgo_econ_action_preview' in x)
                if inspect_elem:
                    inspect_link = inspect_elem.get('href')
                    logger.info(f"   Inspect —Å—Å—ã–ª–∫–∞ –Ω–∞–π–¥–µ–Ω–∞: {inspect_link}")
                else:
                    logger.warning("   ‚ö†Ô∏è Inspect —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ")
                    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ JavaScript
                    scripts = listing_row.find_all('script')
                    for script in scripts:
                        if script.string and 'csgo_econ_action_preview' in script.string:
                            logger.info(f"   Inspect —Å—Å—ã–ª–∫–∞ –Ω–∞–π–¥–µ–Ω–∞ –≤ script: {script.string[:200]}...")
            else:
                logger.warning(f"‚ùå –≠–ª–µ–º–µ–Ω—Ç div#listing_{target_listing_id} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ HTML")
                
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–ª–∞—Å—Å—É
                listing_row = soup.find('div', class_=lambda x: x and f'listing_{target_listing_id}' in x)
                if listing_row:
                    logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —ç–ª–µ–º–µ–Ω—Ç —Å –∫–ª–∞—Å—Å–æ–º listing_{target_listing_id}")
                else:
                    logger.warning(f"‚ùå –≠–ª–µ–º–µ–Ω—Ç —Å –∫–ª–∞—Å—Å–æ–º listing_{target_listing_id} –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    
                    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å listing_id –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
                    all_listing_rows = soup.find_all('div', class_='market_listing_row')
                    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(all_listing_rows)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ market_listing_row –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ 2")
                    for idx, row in enumerate(all_listing_rows[:5], 1):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 5
                        row_id = row.get('id', '')
                        row_classes = row.get('class', [])
                        logger.info(f"   –õ–æ—Ç [{idx}]: id={row_id}, classes={row_classes}")
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—É
                        price_elem = row.select_one('.market_listing_price_with_fee')
                        if price_elem:
                            price_text = price_elem.get_text(strip=True)
                            logger.info(f"      –¶–µ–Ω–∞: {price_text}")
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º inspect —Å—Å—ã–ª–∫—É
                        inspect_elem = row.find('a', href=lambda x: x and 'csgo_econ_action_preview' in x)
                        if inspect_elem:
                            inspect_link = inspect_elem.get('href')
                            logger.info(f"      Inspect: {inspect_link[:100]}...")
                        else:
                            logger.warning(f"      Inspect —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

if __name__ == "__main__":
    asyncio.run(test_listing_page())

