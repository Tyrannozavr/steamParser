"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–ª–∞ URL'–æ–≤.
–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö URL'–æ–≤ –∏–∑ –ø—É–ª–∞.
"""
import asyncio
import httpx
from typing import List, Dict, Any
from loguru import logger

from ..models import SearchFilters
from ..logger import get_task_logger


async def process_url_pool(
    parser,
    listing_parser,
    url_pool: List[Dict[str, Any]],
    filters: SearchFilters,
    task=None,
    db_session=None,
    redis_service=None
) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ URL'—ã –∏–∑ –ø—É–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ.
    
    Args:
        parser: –≠–∫–∑–µ–º–ø–ª—è—Ä SteamMarketParser –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –µ–≥–æ –º–µ—Ç–æ–¥–æ–≤
        listing_parser: –≠–∫–∑–µ–º–ø–ª—è—Ä ListingParser –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–æ—Ç–æ–≤
        url_pool: –ü—É–ª URL'–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        filters: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    all_items = []
    all_listing_ids = set()
    total_count = 0
    
    # –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è –∑–∞–¥–∞—á–∏ (–µ—Å–ª–∏ task_id —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
    task_logger = get_task_logger()
    
    logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—É–ª–∞ –∏–∑ {len(url_pool)} URL'–æ–≤...")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–ª–∞ –≤ –ª–æ–≥ –∑–∞–¥–∞—á–∏
    try:
        if task_logger and task_logger.task_id:
            task_logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—É–ª–∞ –∏–∑ {len(url_pool)} URL'–æ–≤...")
    except Exception:
        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Å task_logger
    
    for idx, url_info in enumerate(url_pool):
        url_type = url_info["type"]
        url = url_info["url"]
        params = url_info["params"]
        page = url_info.get("page", 1)
        total_pages = url_info.get("total_pages", 1)
        
        logger.info(f"üìÑ [{idx + 1}/{len(url_pool)}] –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {url_type} URL (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{total_pages})...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –ª–æ–≥ –∑–∞–¥–∞—á–∏
        try:
            if task_logger and task_logger.task_id:
                if url_type == "query":
                    task_logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º query —Å—Ç—Ä–∞–Ω–∏—Ü—É {page} –∏–∑ {total_pages} (URL {idx + 1}/{len(url_pool)})...")
                elif url_type == "direct":
                    task_logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä—è–º—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–º–µ—Ç–∞ (URL {idx + 1}/{len(url_pool)})...")
                else:
                    task_logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {url_type} URL (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}/{total_pages}, URL {idx + 1}/{len(url_pool)})...")
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Å task_logger
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if idx > 0:
            await parser._random_delay(min_seconds=1.0, max_seconds=2.0)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ 429 –æ—à–∏–±–∫–∞—Ö
        max_retries = 3
        max_proxy_switches = 10
        proxy_switches = 0
        attempt = 0
        data = None
        
        while attempt < max_retries:
            try:
                # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
                headers = parser._get_browser_headers()
                parser._client.headers.update(headers)
                
                # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
                if url_type == "query":
                    response = await parser._client.get(url, params=params)
                else:  # direct
                    full_url = url + "?" + "&".join([f"{k}={v}" for k, v in params.items()])
                    logger.debug(f"üîç Direct URL –∑–∞–ø—Ä–æ—Å: {full_url}")
                    response = await parser._client.get(full_url)
                    logger.debug(f"‚úÖ Direct URL –æ—Ç–≤–µ—Ç: status_code={response.status_code}")
                
                response.raise_for_status()
                data = response.json()
                logger.debug(f"üì• –î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã: success={data.get('success')}, total_count={data.get('total_count')}, results_len={len(data.get('results', []))}, results_html_len={len(data.get('results_html', ''))}")
                break  # –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª–∏ –¥–∞–Ω–Ω—ã–µ
                
            except httpx.HTTPStatusError as e:
                if e.response.status_code == 429:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º 429 –æ—à–∏–±–∫—É
                    current_proxy = await parser._get_current_proxy()
                    await parser._handle_429_fast(current_proxy, f"'{filters.item_name}' (–ø—É–ª URL'–æ–≤)")
                    
                    # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É
                    if proxy_switches < max_proxy_switches:
                        proxy_switched = await parser._switch_proxy()
                        if proxy_switched:
                            proxy_switches += 1
                            logger.info(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–∫—Å–∏ {proxy_switches}/{max_proxy_switches} –∏–∑-–∑–∞ 429, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –Ω–æ–≤—ã–º –ø—Ä–æ–∫—Å–∏")
                            attempt = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–∏ –ø—Ä–æ–∫—Å–∏
                            await asyncio.sleep(1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                            continue
                        else:
                            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–∫—Å–∏, –ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}")
                            attempt += 1
                            if attempt < max_retries:
                                await asyncio.sleep(2)
                                continue
                    else:
                        logger.error(f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–π –ø—Ä–æ–∫—Å–∏ ({max_proxy_switches})")
                        break
                else:
                    # –î—Ä—É–≥–∞—è HTTP –æ—à–∏–±–∫–∞
                    logger.error(f"‚ùå HTTP –æ—à–∏–±–∫–∞ {e.response.status_code} –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL [{idx + 1}/{len(url_pool)}]")
                    break
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ URL [{idx + 1}/{len(url_pool)}]: {e}")
                attempt += 1
                if attempt < max_retries:
                    await asyncio.sleep(1)
                    continue
                break
        
        if not data:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è URL [{idx + 1}/{len(url_pool)}] –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫")
            continue
        
        try:
            if data.get("success"):
                if url_type == "query":
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã query –∑–∞–ø—Ä–æ—Å–∞
                    items = data.get("results", [])
                    total_count = max(total_count, data.get("total_count", 0))
                    
                    for item in items:
                        listing_id = item.get("listingid")
                        if listing_id and listing_id not in all_listing_ids:
                            all_listing_ids.add(listing_id)
                            all_items.append(item)
                        elif not listing_id:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏ —Ü–µ–Ω–µ
                            item_name = item.get('name', item.get('asset_description', {}).get('market_hash_name', ''))
                            item_price = item.get('sell_price_text', '').replace('$', '').replace(',', '').strip()
                            is_duplicate = False
                            for existing_item in all_items:
                                existing_name = existing_item.get('name', existing_item.get('asset_description', {}).get('market_hash_name', ''))
                                existing_price = existing_item.get('sell_price_text', '').replace('$', '').replace(',', '').strip()
                                if item_name == existing_name and item_price == existing_price:
                                    is_duplicate = True
                                    break
                            if not is_duplicate:
                                all_items.append(item)
                    
                    logger.info(f"‚úÖ Query —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ø–æ–ª—É—á–µ–Ω–æ {len(items)} –ø—Ä–µ–¥–º–µ—Ç–æ–≤, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(all_items)}")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ª–æ–≥ –∑–∞–¥–∞—á–∏
                    try:
                        if task_logger and task_logger.task_id:
                            task_logger.info(f"‚úÖ Query —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page} –∏–∑ {total_pages}: –ø–æ–ª—É—á–µ–Ω–æ {len(items)} –ø—Ä–µ–¥–º–µ—Ç–æ–≤, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(all_items)}")
                    except Exception:
                        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Å task_logger
                else:  # direct
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä—è–º–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    hash_name = url_info.get("hash_name", "")
                    total_count = max(total_count, data.get("total_count", 0))
                    
                    # –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º _parse_all_listings –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ render API
                    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –º—ã –ø–æ–ª—É—á–∏–º –≤—Å–µ –ª–æ—Ç—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    logger.info(f"üîç –ü–∞—Ä—Å–∏–º –ø—Ä—è–º—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ _parse_all_listings –¥–ª—è '{hash_name}'...")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä—è–º–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ª–æ–≥ –∑–∞–¥–∞—á–∏
                    try:
                        if task_logger and task_logger.task_id:
                            task_logger.info(f"üîç –ü–∞—Ä—Å–∏–º –ø—Ä—è–º—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø—Ä–µ–¥–º–µ—Ç–∞ '{hash_name}'...")
                    except Exception:
                        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Å task_logger
                    
                    # –í–ê–ñ–ù–û: –§–æ—Ä–º–∏—Ä—É–µ–º target_patterns –∏–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –¥–ª—è —Ä–∞–Ω–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                    target_patterns = None
                    if filters.pattern_list:
                        target_patterns = set(filters.pattern_list.patterns)
                        logger.info(f"    üéØ –§–∏–ª—å—Ç—Ä –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É (direct): –∏—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã {target_patterns}")
                    elif filters.pattern_range:
                        target_patterns = set(range(filters.pattern_range.min, filters.pattern_range.max + 1))
                        logger.info(f"    üéØ –§–∏–ª—å—Ç—Ä –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É (direct): –∏—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ {filters.pattern_range.min}-{filters.pattern_range.max}")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º listing_parser –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                    parsed_listings = await listing_parser.parse_all_listings(
                        filters.appid,
                        hash_name,
                        filters,
                        target_patterns=target_patterns,
                        task_logger=task_logger,
                        task=task,
                        db_session=db_session,
                        redis_service=redis_service
                    )
                    
                    logger.info(f"‚úÖ _parse_all_listings –≤–µ—Ä–Ω—É–ª {len(parsed_listings)} –ª–æ—Ç–æ–≤ –¥–ª—è '{hash_name}'")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä—è–º–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ª–æ–≥ –∑–∞–¥–∞—á–∏
                    if task_logger and task_logger.task_id:
                        task_logger.info(f"‚úÖ –ü—Ä—è–º–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞: –ø–æ–ª—É—á–µ–Ω–æ {len(parsed_listings)} –ª–æ—Ç–æ–≤ –¥–ª—è '{hash_name}'")
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ParsedItemData –≤ —Ñ–æ—Ä–º–∞—Ç item –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    logger.info(f"üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º {len(parsed_listings)} ParsedItemData –≤ —Ñ–æ—Ä–º–∞—Ç items...")
                    for parsed_item in parsed_listings:
                        listing_id = parsed_item.listing_id
                        logger.debug(f"   üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ParsedItemData: listing_id={listing_id}, price={parsed_item.item_price}, pattern={parsed_item.pattern}")
                        
                        if listing_id and listing_id not in all_listing_ids:
                            all_listing_ids.add(listing_id)
                            item = {
                                'name': hash_name,
                                'asset_description': {'market_hash_name': hash_name},
                                'sell_price_text': f"${parsed_item.item_price:.2f}" if parsed_item.item_price else "$0.00",
                                'listingid': listing_id,
                                'parsed_data': {
                                    'item_price': parsed_item.item_price,
                                    'float_value': parsed_item.float_value,
                                    'pattern': parsed_item.pattern,
                                    'stickers': parsed_item.stickers,
                                    'listing_id': listing_id
                                }
                            }
                            all_items.append(item)
                            logger.info(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω item –≤ all_items: listing_id={listing_id}, price=${parsed_item.item_price:.2f}, pattern={parsed_item.pattern}")
                        elif not listing_id:
                            logger.warning(f"   ‚ö†Ô∏è ParsedItemData –±–µ–∑ listing_id, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: price=${parsed_item.item_price:.2f}, pattern={parsed_item.pattern}")
                        elif listing_id in all_listing_ids:
                            logger.debug(f"   ‚è≠Ô∏è listing_id={listing_id} —É–∂–µ –≤ all_listing_ids, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç")
                    
                    logger.info(f"‚úÖ –ü—Ä—è–º–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ø–æ–ª—É—á–µ–Ω–æ {len(parsed_listings)} –ª–æ—Ç–æ–≤, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(all_items)}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö URL [{idx + 1}/{len(url_pool)}] ({url_type}): {e}", exc_info=True)
            import traceback
            logger.debug(f"Traceback: {traceback.format_exc()}")
            continue
    
    logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ø–æ–ª—É—á–µ–Ω–æ {len(all_items)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏–∑ {total_count} –≤—Å–µ–≥–æ")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—É–ª–∞ –≤ –ª–æ–≥ –∑–∞–¥–∞—á–∏
    try:
        if task_logger and task_logger.task_id:
            task_logger.info(f"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—É–ª–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –ø–æ–ª—É—á–µ–Ω–æ {len(all_items)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–º–µ—Ç–æ–≤ –∏–∑ {total_count} –≤—Å–µ–≥–æ (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(url_pool)} URL'–æ–≤)")
    except Exception:
        pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Å task_logger
    
    return {
        "success": True,
        "total_count": total_count,
        "results": all_items
    }

