"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –ª–æ—Ç–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Redis –æ—á–µ—Ä–µ–¥—å –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –º–µ–∂–¥—É –≤–æ—Ä–∫–µ—Ä–∞–º–∏.
"""
import asyncio
import json
from typing import Optional, List, Set
from datetime import datetime
from loguru import logger

from ..models import SearchFilters, ParsedItemData
from core.steam_market_parser.logger_utils import log_both
from core.steam_market_parser.page_range_optimizer import build_optimized_pages_list
from .parallel_listing_utils import get_available_proxies, get_random_proxy
from .parallel_listing_worker import process_page_from_queue


async def parse_listings_parallel(
    parser,
    appid: int,
    hash_name: str,
    filters: SearchFilters,
    target_patterns: Optional[Set[int]],
    listings_per_page: int,
    total_count: int,
    active_proxies_count: int,
    task_logger=None,
    task=None,
    db_session=None,
    redis_service=None,
    db_manager=None
) -> List[ParsedItemData]:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ª–æ—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Redis –æ—á–µ—Ä–µ–¥–∏.
    –í–æ—Ä–∫–µ—Ä—ã –±–µ—Ä—É—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ (—Å –ø–µ—Ä–≤–æ–π –ø–æ –ø–æ—Å–ª–µ–¥–Ω—é—é).
    
    Args:
        parser: –≠–∫–∑–µ–º–ø–ª—è—Ä SteamMarketParser
        appid: ID –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        hash_name: –•—ç—à-–∏–º—è –ø—Ä–µ–¥–º–µ—Ç–∞
        filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
        target_patterns: –¶–µ–ª–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        listings_per_page: –õ–æ—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É (20)
        total_count: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ—Ç–æ–≤
        active_proxies_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
        task_logger: –õ–æ–≥–≥–µ—Ä –∑–∞–¥–∞—á–∏
        task: –ó–∞–¥–∞—á–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        db_session: –°–µ—Å—Å–∏—è –ë–î
        redis_service: –°–µ—Ä–≤–∏—Å Redis
        db_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –ë–î
        
    Returns:
        –°–ø–∏—Å–æ–∫ ParsedItemData
    """
    def log(level: str, message: str):
        log_both(level, message, task_logger)
    
    log("info", f"üöÄ parse_listings_parallel: –ù–∞—á–∞–ª–æ (total_count={total_count}, active_proxies={active_proxies_count}, redis_service={redis_service is not None})")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    pages_to_fetch = await build_optimized_pages_list(
        parser=parser,
        appid=appid,
        hash_name=hash_name,
        filters=filters,
        total_count=total_count,
        listings_per_page=listings_per_page,
        log_func=log
    )
    
    if not pages_to_fetch:
        log("info", "üìÑ –ù–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü –ª–æ—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
        return []
    
    total_pages = len(pages_to_fetch)
    log("info", f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ª–æ—Ç–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {total_pages} (–≤—Å–µ–≥–æ –ª–æ—Ç–æ–≤: {total_count})")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
    available_proxies = await get_available_proxies(parser, log)
    if not available_proxies:
        log("error", "‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏")
        return []
    
    log("info", f"üåê –î–æ—Å—Ç—É–ø–Ω–æ –ø—Ä–æ–∫—Å–∏: {len(available_proxies)}")
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ n = proxies_count / 3 –∑–∞–ø—Ä–æ—Å–æ–≤
    max_concurrent = max(1, len(available_proxies) // 3)
    log("info", f"üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –º–∞–∫—Å–∏–º—É–º {max_concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤ (–∏–∑ {len(available_proxies)} –ø—Ä–æ–∫—Å–∏)")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Redis –¥–ª—è –æ—á–µ—Ä–µ–¥–∏
    log("debug", f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º Redis: redis_service={redis_service is not None}, is_connected={redis_service.is_connected() if redis_service else False}")
    if not redis_service:
        log("error", "‚ùå Redis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ —Å—Ç—Ä–∞–Ω–∏—Ü: redis_service is None")
        return []
    if not redis_service.is_connected():
        log("error", "‚ùå Redis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ —Å—Ç—Ä–∞–Ω–∏—Ü: redis_service –Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω")
        try:
            await redis_service.connect()
            log("info", "‚úÖ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω")
        except Exception as e:
            log("error", f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Redis: {e}")
            return []
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–ª—é—á –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏
    queue_key = f"parsing:pages:task_{task.id if task else 'unknown'}"
    log("info", f"üìã –°–æ–∑–¥–∞–µ–º Redis –æ—á–µ—Ä–µ–¥—å —Å—Ç—Ä–∞–Ω–∏—Ü: {queue_key}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ Redis –æ—á–µ—Ä–µ–¥—å (–≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ–±—ã –±—Ä–∞—Ç—å —Å –ø–µ—Ä–≤–æ–π)
    try:
        log("info", f"üì• –î–æ–±–∞–≤–ª—è–µ–º {len(pages_to_fetch)} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ Redis –æ—á–µ—Ä–µ–¥—å...")
        page_data_list = []
        for page_num, page_start, page_count in pages_to_fetch:
            page_data = json.dumps({
                "page_num": page_num,
                "page_start": page_start,
                "page_count": page_count,
                "appid": appid,
                "hash_name": hash_name
            })
            page_data_list.append(page_data)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –æ—á–µ—Ä–µ–¥—å (LPUSH –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ –Ω–∞—á–∞–ª–æ, –ø–æ—ç—Ç–æ–º—É –¥–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ)
        await redis_service.lpush(queue_key, *reversed(page_data_list))
        queue_length = await redis_service.llen(queue_key)
        log("info", f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(pages_to_fetch)} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ—á–µ—Ä–µ–¥—å (–¥–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏: {queue_length})")
    except Exception as e:
        log("error", f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ –æ—á–µ—Ä–µ–¥—å: {e}")
        import traceback
        log("error", f"   Traceback: {traceback.format_exc()}")
        return []
    
    # –í–ê–ñ–ù–û: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–ø–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ Redis (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    # –í –∫–æ–Ω—Ü–µ —Å–æ–±–µ—Ä–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ Redis
    matching_listings = []
    max_retries = 3  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
    task_start_times = {}  # page_num -> start_time
    task_stages = {}  # page_num -> current_stage
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ—Ä–∫–µ—Ä—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    log("info", f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º {max_concurrent} –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ Redis –æ—á–µ—Ä–µ–¥–∏...")
    
    workers = [
        asyncio.create_task(
            process_page_from_queue(
                worker_id=worker_id,
                queue_key=queue_key,
                redis_service=redis_service,
                parser=parser,
                appid=appid,
                hash_name=hash_name,
                filters=filters,
                task=task,
                db_manager=db_manager,
                task_logger=task_logger,
                redis_service_for_notifications=redis_service,
                available_proxies=available_proxies,
                max_retries=max_retries,
                total_pages=total_pages,
                task_start_times=task_start_times,
                task_stages=task_stages,
                log_func=log
            )
        )
        for worker_id in range(1, max_concurrent + 1)
    ]
    
    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
    try:
        await asyncio.gather(*workers, return_exceptions=True)
    except Exception as e:
        log("error", f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –≤–æ—Ä–∫–µ—Ä–æ–≤: {e}")
        import traceback
        log("error", f"   Traceback: {traceback.format_exc()}")
    
    # –û—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    try:
        await redis_service.delete(queue_key)
        log("info", f"üóëÔ∏è –û—á–µ—Ä–µ–¥—å {queue_key} –æ—á–∏—â–µ–Ω–∞")
    except Exception as e:
        log("warning", f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –æ—á–µ—Ä–µ–¥—å {queue_key}: {e}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–∏—Å—à–∏–µ –∑–∞–¥–∞—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if task_start_times:
        now = datetime.now()
        hung_pages = []
        for page_num, start_time in task_start_times.items():
            elapsed = (now - start_time).total_seconds()
            if elapsed > 300:  # 5 –º–∏–Ω—É—Ç
                current_stage = task_stages.get(page_num, "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")
                hung_pages.append((page_num, elapsed, current_stage))
        
        if hung_pages:
            log("error", f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–∞–≤–∏—Å—à–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:")
            for page_num, elapsed, stage in hung_pages:
                log("error", f"   üìã –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}: –∑–∞–≤–∏—Å–ª–∞ –Ω–∞ —ç—Ç–∞–ø–µ '{stage}' —É–∂–µ {elapsed:.1f}—Å")
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ Redis (–±—ã—Å—Ç—Ä–æ, –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    # –í–ê–ñ–ù–û: –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –ø–∞—Ä—Å–µ—Ä–µ (—Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è),
    # —Ç–æ –æ–Ω–∏ –Ω–µ –±—É–¥—É—Ç –≤ Redis, —Ç–∞–∫ –∫–∞–∫ process_item_result —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–ª –∏—Ö
    try:
        from .parallel_listing_redis_storage import get_all_results_from_redis, cleanup_redis_results
        
        log("info", f"üì• –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ Redis...")
        matching_listings = await get_all_results_from_redis(
            redis_service=redis_service,
            task_id=task.id if task else 0,
            total_pages=total_pages,
            log_func=log
        )
        
        # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ Redis –ø–æ—Å–ª–µ —Å–±–æ—Ä–∞
        await cleanup_redis_results(
            redis_service=redis_service,
            task_id=task.id if task else 0,
            total_pages=total_pages,
            log_func=log
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ Redis
        completed_count = 0
        if task:
            completed_key = f"parsing:completed:task_{task.id}"
            try:
                completed_str = await redis_service.get(completed_key)
                if completed_str:
                    completed_count = int(completed_str)
                # –û—á–∏—â–∞–µ–º —Å—á–µ—Ç—á–∏–∫
                await redis_service.delete(completed_key)
            except Exception:
                pass
        
        log("info", f"üìä –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {completed_count}/{len(pages_to_fetch)} —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–∞–π–¥–µ–Ω–æ {len(matching_listings)} –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ª–æ—Ç–æ–≤")
    except Exception as e:
        log("error", f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ Redis: {e}")
        import traceback
        log("error", f"   Traceback: {traceback.format_exc()}")
    if len(matching_listings) == 0:
        log("info", f"‚ÑπÔ∏è –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—É—Å—Ç - –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–º–µ—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —Å—Ä–∞–∑—É (—É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã)")
    
    return matching_listings
